# shape_pers <- (1/(4*var_pers))-.5
# shape_trans <- (1/(4*var_trans))-.5
# df <- tibble(pers = rbeta(n, shape_pers, shape_pers),
#              trans = rbeta(n, shape_trans, shape_trans))
# with the t distribution
#
# df_pers <- 3
# df_trans <- 4
# df <- tibble(pers = rt(n, df_pers),
#              trans = rt(n, df_trans))
df <- df %>%
mutate(y = pers +trans)
df$y_rank[order(df$y)] <- 1:nrow(df)
df$pers_rank[order(df$pers)] <- 1:nrow(df)
df <- df %>%
mutate(top_cent_pers = (pers_rank > (1-centile)*n), top_cent_y = (y_rank > (1-centile)*n))
# Proportion of those ranking in the top which should be ranking in top quartile
mean(df$top_cent_pers[df$top_cent_y])
library(tidyverse)
dist_path <- "C:/Users/dougj/Documents/Data/Education/ASER District Data/Clean"
output <- "C:/Users/dougj/Dropbox/Education in India/Original research/Learning outcomes data/figures"
dists <- read_csv(file.path(dist_path, "aser_district_partial.csv"))
state_path <- "C:/Users/dougj/Documents/Data/Education/ASER trends over time"
states <- read_csv(file.path(state_path, "aser_trends.csv")) %>% select(year, State, std3_reading, std3_math)
library(tidyverse)
dist_path <- "C:/Users/dougj/Documents/Data/Education/ASER District Data/Clean"
output <- "C:/Users/dougj/Dropbox/Education in India/Original research/Learning outcomes data/figures"
dists <- read_csv(file.path(dist_path, "aser_district_partial.csv"))
state_path <- "C:/Users/dougj/Documents/Data/Education/ASER trends over time"
states <- read_csv(file.path(state_path, "aser_trends.csv")) %>%
select(year, State, std3_read_std1_all, std3_subtract_all) %>%
rename(std3_reading = std3_read_std1_all, std3_math = std3_subtract_all)
##### IMPORTANT: TO DO ALL THE ANALYZE FOR MATH, SET THE SUBJECT BELOW TO "MATH" ###
subject <- "math"
if (subject == "math") {
dists$std35_std1_and_up <- dists$std35_subtraction_and_up
states$std3_reading <- states$std3_math
}
### IMPORTANT -- THIS IS WHERE I SPECIFY DISTRICT SAMPLING VARIANCE ###
var_dist_sampling <- .0013
# Create variable for deltas and first and second lag of the delta
dist_deltas <- dists %>%
group_by(State, District) %>%
mutate(delta_reading = std35_std1_and_up - lag(std35_std1_and_up, order_by = year), delta_math = std35_subtraction_and_up - lag(std35_subtraction_and_up)) %>%
mutate(delta_reading_lagged = lag(delta_reading, order_by = year), delta_reading_dbl_lag = lag(delta_reading, n= 2, order_by = year)) %>%
arrange(State, District, year)
# Calculate autocorrelation of delta_reading by district and then average over all districts
# Note that since the data is already grouped by State and district, we are calculating
# autocorrelation for each district and then averaging across all districts
corr_dist_deltas <- dist_deltas %>%
filter(year >= 2008) %>%
summarise(auto = cor(delta_reading, delta_reading_lagged)) %>%
ungroup() %>%
summarise(mean_auto=mean(auto)) %>% .$mean_auto
# Calculate the autocorrelation in one go for all pairs of delta and delta_lagged.
# Note that we are not doing any averaging here.
# This should spit out a very similar value and it does.
check_corr_dist_deltas <- dist_deltas %>%
ungroup() %>%
filter(year >= 2008) %>%
summarise(auto = cor(delta_reading, delta_reading_lagged))
# Calculate the overall variance of the deltas
# Note that we have to divide by 100 here to get the actual number.
# We didn't need to do this above because we just calculated correlation and thus scale didn't matter
var_dist_deltas <- var(dist_deltas$delta_reading/100, na.rm = TRUE)
# Calculate overall variance of the district values.
# Note that we don't want to first calculate variance of dist reading scores by state and then average
# That would really underestimate the variance of the district scores.
# (To understand intuition, note districts are considered iid in the model above.)
var_dist_levels <- var(dists$std35_std1_and_up/100, na.rm = TRUE)
# Calculate variance of the transitory component
var_dist_epsilon <- -corr_dist_deltas*var_dist_deltas
#
corr_dist_dbl_lag <- dist_deltas %>% filter(year >= 2009) %>%
ungroup() %>%
summarize(auto =cor(delta_reading,delta_reading_dbl_lag, use = "pairwise.complete.obs")) %>%
.$auto
# Check that changes in letters and numbers is correlated -- they are
dist_deltas %>% filter(year != 2006) %>% ungroup() %>%
summarize(cor(delta_reading,delta_math))
# Create variable for year on year change for each district
state_deltas <- states %>%
group_by(State) %>%
mutate(delta_reading = std3_reading - lag(std3_reading, order_by = year), delta_math = std3_math - lag(std3_math, order_by = year)) %>%
mutate(delta_reading_lagged = lag(delta_reading, order_by = year), delta_reading_dbl_lag = lag(delta_reading,n=2, order_by = year)) %>%
arrange(State, year)
# Calculate average autocorrelation of delta_reading
corr_state_deltas <- state_deltas %>%
filter(year >= 2008) %>%
summarise(auto = cor(delta_reading, delta_reading_lagged, use = "pairwise.complete.obs")) %>%
ungroup() %>%
summarise(mean_auto=mean(auto)) %>% .$mean_auto
# Calculate correlation betwen delta letters and delta letters lagged for entire dataset (without first calculating for each state and then averaging).
# this should be about the same as when we first calculate for each state and then average --> which it is.
check_corr_state_deltas <- state_deltas %>% filter(year >= 2008) %>%
ungroup() %>%
summarize(auto =cor(delta_reading,delta_reading_lagged, use = "pairwise.complete.obs")) %>%
.$auto
# calculate the variance of the deltas
var_state_deltas <- var(state_deltas$delta_reading/100, na.rm = TRUE)
# calculate the variance of the levels
var_state_levels <- var(state_deltas$std3_reading/100, na.rm = TRUE)
# calculate variance of transitory component
var_state_epsilon <- -corr_state_deltas*var_state_deltas
var_state_sampling <- dists %>% filter(year ==2008) %>%
count(State) %>%
mutate(sampling_var = var_dist_sampling/n) %>%
summarize(mean_sampling = mean(sampling_var)) %>% .$mean_sampling
# Check that changes in letters and numbers is correlated -- they are highly correlated
state_deltas %>% filter(year >= 2008) %>% ungroup() %>%
summarize(cor(delta_reading,delta_math, use = "pairwise.complete.obs"))
corr_state_dbl_lag <- state_deltas %>% filter(year >= 2009) %>%
ungroup() %>%
summarize(auto =cor(delta_reading,delta_reading_dbl_lag, use = "pairwise.complete.obs")) %>%
.$auto
temp <- state_deltas %>% filter(year >= 2009) %>%
group_by(State) %>%
summarize(auto =cor(delta_reading,delta_reading_dbl_lag, use = "pairwise.complete.obs"))
state_lags <- states %>%
group_by(State) %>%
mutate(read_lag1 = lag(std3_reading, order_by = year),
read_lag2 = lag(std3_reading, n= 2, order_by = year),
read_lag3 = lag(std3_reading, n =3, order_by = year),
read_lag4 = lag(std3_reading, n =4, order_by = year),
read_lag5 = lag(std3_reading, n =5, order_by = year)) %>%
ungroup()
rho = c()
# for each of the lags, calculate the correlation between current and the lag and store in vector rho
for (lag in seq(1,5)) {
rho <- c(rho, cor(state_lags$std3_reading, state_lags[[paste("read_lag",as.character(lag),sep="")]], use = "pairwise.complete.obs"))
}
# Calculate variance of the transitory component by comparing rho_1 with the decay of rho after that
var_state_levels_persistent2 <- rho[1]/mean(rho/lag(rho), na.rm = TRUE)*var_state_levels
# Calculate the share of variance coming from other transitory sources.
# We will need this to calculate the variance breakup for changes
var_state_levels_other2 <- var_state_levels - var_state_levels_persistent2 - var_state_sampling
# Save a graph showing the decay of the correlation
quick_df <- tibble(lag = seq(0,5), correlation = c(1,rho))
ggplot(quick_df, aes(x= lag, y = correlation)) +
geom_line() +
geom_point() +
ylim(0,1)
ggsave(paste(subject, "- correlation_decay.png"), width = 5, height = 6 , path = output)
# Create empty tibble to store results
# I
df_bar <- tibble(state_or_dist = rep(c("State","District"),each =6),
changes_or_levels =rep(rep(c("Changes","Levels"), each =3), times= 2),
bar_part = factor(rep(c("Persistent","Transitory sampling", "Transitory other"),4), levels = c("Transitory other","Transitory sampling", "Persistent")),
value = rep(.001,12)
)
# Update the values for the district changes bar
df_bar$value[(df_bar$state_or_dist == "District") & (df_bar$changes_or_levels == "Changes") & (df_bar$bar_part == "Persistent")] <- var_dist_deltas - 2*var_dist_epsilon
df_bar$value[(df_bar$state_or_dist == "District") & (df_bar$changes_or_levels == "Changes") & (df_bar$bar_part == "Transitory sampling")] <- 2*var_dist_sampling
df_bar$value[(df_bar$state_or_dist == "District") & (df_bar$changes_or_levels == "Changes") & (df_bar$bar_part == "Transitory other")] <- 2*var_dist_epsilon - 2*var_dist_sampling
# Update the values for the district levels bar
df_bar$value[(df_bar$state_or_dist == "District") & (df_bar$changes_or_levels == "Levels") & (df_bar$bar_part == "Persistent")] <- var_dist_levels - var_dist_epsilon
df_bar$value[(df_bar$state_or_dist == "District") & (df_bar$changes_or_levels == "Levels") & (df_bar$bar_part == "Transitory sampling")] <- var_dist_sampling
df_bar$value[(df_bar$state_or_dist == "District") & (df_bar$changes_or_levels == "Levels") & (df_bar$bar_part == "Transitory other")] <- var_dist_epsilon - var_dist_sampling
##### STATE DATA #####
# CHANGES
df_bar$value[(df_bar$state_or_dist == "State") & (df_bar$changes_or_levels == "Changes") & (df_bar$bar_part == "Persistent")] <- var_state_deltas-2*(var_state_sampling+var_state_levels_other2)
df_bar$value[(df_bar$state_or_dist == "State") & (df_bar$changes_or_levels == "Changes") & (df_bar$bar_part == "Transitory sampling")] <- 2*var_state_sampling
df_bar$value[(df_bar$state_or_dist == "State") & (df_bar$changes_or_levels == "Changes") & (df_bar$bar_part == "Transitory other")] <- 2*var_state_levels_other2
# LEVELS
df_bar$value[(df_bar$state_or_dist == "State") & (df_bar$changes_or_levels == "Levels") & (df_bar$bar_part == "Persistent")] <- var_state_levels_persistent2
df_bar$value[(df_bar$state_or_dist == "State") & (df_bar$changes_or_levels == "Levels") & (df_bar$bar_part == "Transitory sampling")] <- var_state_sampling
df_bar$value[(df_bar$state_or_dist == "State") & (df_bar$changes_or_levels == "Levels") & (df_bar$bar_part == "Transitory other")] <- var_state_levels_other2
# I have created this graph so that you can see it in the notebook
# The final version which combines both subjects, is created in a separate notebook
ggplot(df_bar, aes(fill = bar_part, y=value, x= changes_or_levels)) +
geom_bar(position="stack", stat ="identity") +
facet_grid(~ state_or_dist) +
scale_fill_manual(values = c("red", "orange", "blue"))+
labs(fill = "Variance component", x = "")
ggsave(paste(subject," - variance_decomposition.png"), width = 5, height = 6 , path = output)
write_csv(df_bar, file.path(output, paste(subject, "- bar data.csv")))
df_final <- df_bar %>%
pivot_wider(names_from = "bar_part", values_from= "value") %>%
mutate(total_var = `Persistent`+`Transitory sampling`+`Transitory other`) %>%
mutate(share_pers = `Persistent`/total_var, share_samp = `Transitory sampling`/total_var, share_other = `Transitory other`/total_var)
write_csv(df_final, file.path(output, paste(subject, "- var deco.csv")))
library(tidyverse)
dist_path <- "C:/Users/dougj/Documents/Data/Education/ASER District Data/Clean"
output <- "C:/Users/dougj/Dropbox/Education in India/Original research/Learning outcomes data/figures"
dists <- read_csv(file.path(dist_path, "aser_district_partial.csv"))
state_path <- "C:/Users/dougj/Documents/Data/Education/ASER trends over time"
states <- read_csv(file.path(state_path, "aser_trends.csv")) %>%
select(year, State, std3_read_std1_all, std3_subtract_all) %>%
rename(std3_reading = std3_read_std1_all, std3_math = std3_subtract_all)
##### IMPORTANT: TO DO ALL THE ANALYZE FOR MATH, SET THE SUBJECT BELOW TO "MATH" ###
subject <- "math"
if (subject == "math") {
dists$std35_std1_and_up <- dists$std35_subtraction_and_up
states$std3_reading <- states$std3_math
}
### IMPORTANT -- THIS IS WHERE I SPECIFY DISTRICT SAMPLING VARIANCE ###
var_dist_sampling <- .0013
# Create variable for deltas and first and second lag of the delta
dist_deltas <- dists %>%
group_by(State, District) %>%
mutate(delta_reading = std35_std1_and_up - lag(std35_std1_and_up, order_by = year), delta_math = std35_subtraction_and_up - lag(std35_subtraction_and_up)) %>%
mutate(delta_reading_lagged = lag(delta_reading, order_by = year), delta_reading_dbl_lag = lag(delta_reading, n= 2, order_by = year)) %>%
arrange(State, District, year)
# Calculate autocorrelation of delta_reading by district and then average over all districts
# Note that since the data is already grouped by State and district, we are calculating
# autocorrelation for each district and then averaging across all districts
corr_dist_deltas <- dist_deltas %>%
filter(year >= 2008) %>%
summarise(auto = cor(delta_reading, delta_reading_lagged)) %>%
ungroup() %>%
summarise(mean_auto=mean(auto)) %>% .$mean_auto
# Calculate the autocorrelation in one go for all pairs of delta and delta_lagged.
# Note that we are not doing any averaging here.
# This should spit out a very similar value and it does.
check_corr_dist_deltas <- dist_deltas %>%
ungroup() %>%
filter(year >= 2008) %>%
summarise(auto = cor(delta_reading, delta_reading_lagged))
# Calculate the overall variance of the deltas
# Note that we have to divide by 100 here to get the actual number.
# We didn't need to do this above because we just calculated correlation and thus scale didn't matter
var_dist_deltas <- var(dist_deltas$delta_reading/100, na.rm = TRUE)
# Calculate overall variance of the district values.
# Note that we don't want to first calculate variance of dist reading scores by state and then average
# That would really underestimate the variance of the district scores.
# (To understand intuition, note districts are considered iid in the model above.)
var_dist_levels <- var(dists$std35_std1_and_up/100, na.rm = TRUE)
# Calculate variance of the transitory component
var_dist_epsilon <- -corr_dist_deltas*var_dist_deltas
#
corr_dist_dbl_lag <- dist_deltas %>% filter(year >= 2009) %>%
ungroup() %>%
summarize(auto =cor(delta_reading,delta_reading_dbl_lag, use = "pairwise.complete.obs")) %>%
.$auto
# Check that changes in letters and numbers is correlated -- they are
dist_deltas %>% filter(year != 2006) %>% ungroup() %>%
summarize(cor(delta_reading,delta_math))
# Create variable for year on year change for each district
state_deltas <- states %>%
group_by(State) %>%
mutate(delta_reading = std3_reading - lag(std3_reading, order_by = year), delta_math = std3_math - lag(std3_math, order_by = year)) %>%
mutate(delta_reading_lagged = lag(delta_reading, order_by = year), delta_reading_dbl_lag = lag(delta_reading,n=2, order_by = year)) %>%
arrange(State, year)
# Calculate average autocorrelation of delta_reading
corr_state_deltas <- state_deltas %>%
filter(year >= 2008) %>%
summarise(auto = cor(delta_reading, delta_reading_lagged, use = "pairwise.complete.obs")) %>%
ungroup() %>%
summarise(mean_auto=mean(auto)) %>% .$mean_auto
# Calculate correlation betwen delta letters and delta letters lagged for entire dataset (without first calculating for each state and then averaging).
# this should be about the same as when we first calculate for each state and then average --> which it is.
check_corr_state_deltas <- state_deltas %>% filter(year >= 2008) %>%
ungroup() %>%
summarize(auto =cor(delta_reading,delta_reading_lagged, use = "pairwise.complete.obs")) %>%
.$auto
# calculate the variance of the deltas
var_state_deltas <- var(state_deltas$delta_reading, na.rm = TRUE)
# calculate the variance of the levels
var_state_levels <- var(state_deltas$std3_reading, na.rm = TRUE)
# calculate variance of transitory component
var_state_epsilon <- -corr_state_deltas*var_state_deltas
var_state_sampling <- dists %>% filter(year ==2008) %>%
count(State) %>%
mutate(sampling_var = var_dist_sampling/n) %>%
summarize(mean_sampling = mean(sampling_var)) %>% .$mean_sampling
# Check that changes in letters and numbers is correlated -- they are highly correlated
state_deltas %>% filter(year >= 2008) %>% ungroup() %>%
summarize(cor(delta_reading,delta_math, use = "pairwise.complete.obs"))
corr_state_dbl_lag <- state_deltas %>% filter(year >= 2009) %>%
ungroup() %>%
summarize(auto =cor(delta_reading,delta_reading_dbl_lag, use = "pairwise.complete.obs")) %>%
.$auto
temp <- state_deltas %>% filter(year >= 2009) %>%
group_by(State) %>%
summarize(auto =cor(delta_reading,delta_reading_dbl_lag, use = "pairwise.complete.obs"))
state_lags <- states %>%
group_by(State) %>%
mutate(read_lag1 = lag(std3_reading, order_by = year),
read_lag2 = lag(std3_reading, n= 2, order_by = year),
read_lag3 = lag(std3_reading, n =3, order_by = year),
read_lag4 = lag(std3_reading, n =4, order_by = year),
read_lag5 = lag(std3_reading, n =5, order_by = year)) %>%
ungroup()
rho = c()
# for each of the lags, calculate the correlation between current and the lag and store in vector rho
for (lag in seq(1,5)) {
rho <- c(rho, cor(state_lags$std3_reading, state_lags[[paste("read_lag",as.character(lag),sep="")]], use = "pairwise.complete.obs"))
}
# Calculate variance of the transitory component by comparing rho_1 with the decay of rho after that
var_state_levels_persistent2 <- rho[1]/mean(rho/lag(rho), na.rm = TRUE)*var_state_levels
# Calculate the share of variance coming from other transitory sources.
# We will need this to calculate the variance breakup for changes
var_state_levels_other2 <- var_state_levels - var_state_levels_persistent2 - var_state_sampling
# Save a graph showing the decay of the correlation
quick_df <- tibble(lag = seq(0,5), correlation = c(1,rho))
ggplot(quick_df, aes(x= lag, y = correlation)) +
geom_line() +
geom_point() +
ylim(0,1)
ggsave(paste(subject, "- correlation_decay.png"), width = 5, height = 6 , path = output)
# Create empty tibble to store results
# I
df_bar <- tibble(state_or_dist = rep(c("State","District"),each =6),
changes_or_levels =rep(rep(c("Changes","Levels"), each =3), times= 2),
bar_part = factor(rep(c("Persistent","Transitory sampling", "Transitory other"),4), levels = c("Transitory other","Transitory sampling", "Persistent")),
value = rep(.001,12)
)
# Update the values for the district changes bar
df_bar$value[(df_bar$state_or_dist == "District") & (df_bar$changes_or_levels == "Changes") & (df_bar$bar_part == "Persistent")] <- var_dist_deltas - 2*var_dist_epsilon
df_bar$value[(df_bar$state_or_dist == "District") & (df_bar$changes_or_levels == "Changes") & (df_bar$bar_part == "Transitory sampling")] <- 2*var_dist_sampling
df_bar$value[(df_bar$state_or_dist == "District") & (df_bar$changes_or_levels == "Changes") & (df_bar$bar_part == "Transitory other")] <- 2*var_dist_epsilon - 2*var_dist_sampling
# Update the values for the district levels bar
df_bar$value[(df_bar$state_or_dist == "District") & (df_bar$changes_or_levels == "Levels") & (df_bar$bar_part == "Persistent")] <- var_dist_levels - var_dist_epsilon
df_bar$value[(df_bar$state_or_dist == "District") & (df_bar$changes_or_levels == "Levels") & (df_bar$bar_part == "Transitory sampling")] <- var_dist_sampling
df_bar$value[(df_bar$state_or_dist == "District") & (df_bar$changes_or_levels == "Levels") & (df_bar$bar_part == "Transitory other")] <- var_dist_epsilon - var_dist_sampling
##### STATE DATA #####
# CHANGES
df_bar$value[(df_bar$state_or_dist == "State") & (df_bar$changes_or_levels == "Changes") & (df_bar$bar_part == "Persistent")] <- var_state_deltas-2*(var_state_sampling+var_state_levels_other2)
df_bar$value[(df_bar$state_or_dist == "State") & (df_bar$changes_or_levels == "Changes") & (df_bar$bar_part == "Transitory sampling")] <- 2*var_state_sampling
df_bar$value[(df_bar$state_or_dist == "State") & (df_bar$changes_or_levels == "Changes") & (df_bar$bar_part == "Transitory other")] <- 2*var_state_levels_other2
# LEVELS
df_bar$value[(df_bar$state_or_dist == "State") & (df_bar$changes_or_levels == "Levels") & (df_bar$bar_part == "Persistent")] <- var_state_levels_persistent2
df_bar$value[(df_bar$state_or_dist == "State") & (df_bar$changes_or_levels == "Levels") & (df_bar$bar_part == "Transitory sampling")] <- var_state_sampling
df_bar$value[(df_bar$state_or_dist == "State") & (df_bar$changes_or_levels == "Levels") & (df_bar$bar_part == "Transitory other")] <- var_state_levels_other2
# I have created this graph so that you can see it in the notebook
# The final version which combines both subjects, is created in a separate notebook
ggplot(df_bar, aes(fill = bar_part, y=value, x= changes_or_levels)) +
geom_bar(position="stack", stat ="identity") +
facet_grid(~ state_or_dist) +
scale_fill_manual(values = c("red", "orange", "blue"))+
labs(fill = "Variance component", x = "")
ggsave(paste(subject," - variance_decomposition.png"), width = 5, height = 6 , path = output)
write_csv(df_bar, file.path(output, paste(subject, "- bar data.csv")))
df_final <- df_bar %>%
pivot_wider(names_from = "bar_part", values_from= "value") %>%
mutate(total_var = `Persistent`+`Transitory sampling`+`Transitory other`) %>%
mutate(share_pers = `Persistent`/total_var, share_samp = `Transitory sampling`/total_var, share_other = `Transitory other`/total_var)
write_csv(df_final, file.path(output, paste(subject, "- var deco.csv")))
View(state_lags)
View(state_lags)
rho
source('~/code/notebooks/process_data/import ASER trends over time data_temp.R', echo=TRUE)
source('~/code/notebooks/process_data/import ASER trends over time data_temp.R', echo=TRUE)
View(full)
View(full)
library(tidyverse)
dist_path <- "C:/Users/dougj/Documents/Data/Education/ASER District Data/Clean"
output <- "C:/Users/dougj/Dropbox/Education in India/Original research/Learning outcomes data/figures"
dists <- read_csv(file.path(dist_path, "aser_district_partial.csv"))
state_path <- "C:/Users/dougj/Documents/Data/Education/ASER trends over time"
states <- read_csv(file.path(state_path, "aser_trends.csv")) %>%
select(year, State, std3_read_std1_all, std3_subtract_all) %>%
rename(std3_reading = std3_read_std1_all, std3_math = std3_subtract_all)
##### IMPORTANT: TO DO ALL THE ANALYZE FOR MATH, SET THE SUBJECT BELOW TO "MATH" ###
subject <- "math"
if (subject == "math") {
dists$std35_std1_and_up <- dists$std35_subtraction_and_up
states$std3_reading <- states$std3_math
}
### IMPORTANT -- THIS IS WHERE I SPECIFY DISTRICT SAMPLING VARIANCE ###
var_dist_sampling <- .0013
# Create variable for deltas and first and second lag of the delta
dist_deltas <- dists %>%
group_by(State, District) %>%
mutate(delta_reading = std35_std1_and_up - lag(std35_std1_and_up, order_by = year), delta_math = std35_subtraction_and_up - lag(std35_subtraction_and_up)) %>%
mutate(delta_reading_lagged = lag(delta_reading, order_by = year), delta_reading_dbl_lag = lag(delta_reading, n= 2, order_by = year)) %>%
arrange(State, District, year)
# Calculate autocorrelation of delta_reading by district and then average over all districts
# Note that since the data is already grouped by State and district, we are calculating
# autocorrelation for each district and then averaging across all districts
corr_dist_deltas <- dist_deltas %>%
filter(year >= 2008) %>%
summarise(auto = cor(delta_reading, delta_reading_lagged)) %>%
ungroup() %>%
summarise(mean_auto=mean(auto)) %>% .$mean_auto
# Calculate the autocorrelation in one go for all pairs of delta and delta_lagged.
# Note that we are not doing any averaging here.
# This should spit out a very similar value and it does.
check_corr_dist_deltas <- dist_deltas %>%
ungroup() %>%
filter(year >= 2008) %>%
summarise(auto = cor(delta_reading, delta_reading_lagged))
# Calculate the overall variance of the deltas
# Note that we have to divide by 100 here to get the actual number.
# We didn't need to do this above because we just calculated correlation and thus scale didn't matter
var_dist_deltas <- var(dist_deltas$delta_reading/100, na.rm = TRUE)
# Calculate overall variance of the district values.
# Note that we don't want to first calculate variance of dist reading scores by state and then average
# That would really underestimate the variance of the district scores.
# (To understand intuition, note districts are considered iid in the model above.)
var_dist_levels <- var(dists$std35_std1_and_up/100, na.rm = TRUE)
# Calculate variance of the transitory component
var_dist_epsilon <- -corr_dist_deltas*var_dist_deltas
#
corr_dist_dbl_lag <- dist_deltas %>% filter(year >= 2009) %>%
ungroup() %>%
summarize(auto =cor(delta_reading,delta_reading_dbl_lag, use = "pairwise.complete.obs")) %>%
.$auto
# Check that changes in letters and numbers is correlated -- they are
dist_deltas %>% filter(year != 2006) %>% ungroup() %>%
summarize(cor(delta_reading,delta_math))
# Create variable for year on year change for each district
state_deltas <- states %>%
group_by(State) %>%
mutate(delta_reading = std3_reading - lag(std3_reading, order_by = year), delta_math = std3_math - lag(std3_math, order_by = year)) %>%
mutate(delta_reading_lagged = lag(delta_reading, order_by = year), delta_reading_dbl_lag = lag(delta_reading,n=2, order_by = year)) %>%
arrange(State, year)
# Calculate average autocorrelation of delta_reading
corr_state_deltas <- state_deltas %>%
filter(year >= 2008) %>%
summarise(auto = cor(delta_reading, delta_reading_lagged, use = "pairwise.complete.obs")) %>%
ungroup() %>%
summarise(mean_auto=mean(auto)) %>% .$mean_auto
# Calculate correlation betwen delta letters and delta letters lagged for entire dataset (without first calculating for each state and then averaging).
# this should be about the same as when we first calculate for each state and then average --> which it is.
check_corr_state_deltas <- state_deltas %>% filter(year >= 2008) %>%
ungroup() %>%
summarize(auto =cor(delta_reading,delta_reading_lagged, use = "pairwise.complete.obs")) %>%
.$auto
# calculate the variance of the deltas
var_state_deltas <- var(state_deltas$delta_reading, na.rm = TRUE)
# calculate the variance of the levels
var_state_levels <- var(state_deltas$std3_reading, na.rm = TRUE)
# calculate variance of transitory component
var_state_epsilon <- -corr_state_deltas*var_state_deltas
var_state_sampling <- dists %>% filter(year ==2008) %>%
count(State) %>%
mutate(sampling_var = var_dist_sampling/n) %>%
summarize(mean_sampling = mean(sampling_var)) %>% .$mean_sampling
# Check that changes in letters and numbers is correlated -- they are highly correlated
state_deltas %>% filter(year >= 2008) %>% ungroup() %>%
summarize(cor(delta_reading,delta_math, use = "pairwise.complete.obs"))
corr_state_dbl_lag <- state_deltas %>% filter(year >= 2009) %>%
ungroup() %>%
summarize(auto =cor(delta_reading,delta_reading_dbl_lag, use = "pairwise.complete.obs")) %>%
.$auto
temp <- state_deltas %>% filter(year >= 2009) %>%
group_by(State) %>%
summarize(auto =cor(delta_reading,delta_reading_dbl_lag, use = "pairwise.complete.obs"))
state_lags <- states %>%
group_by(State) %>%
mutate(read_lag1 = lag(std3_reading, order_by = year),
read_lag2 = lag(std3_reading, n= 2, order_by = year),
read_lag3 = lag(std3_reading, n =3, order_by = year),
read_lag4 = lag(std3_reading, n =4, order_by = year),
read_lag5 = lag(std3_reading, n =5, order_by = year)) %>%
ungroup()
rho = c()
# for each of the lags, calculate the correlation between current and the lag and store in vector rho
for (lag in seq(1,5)) {
rho <- c(rho, cor(state_lags$std3_reading, state_lags[[paste("read_lag",as.character(lag),sep="")]], use = "pairwise.complete.obs"))
}
# Calculate variance of the transitory component by comparing rho_1 with the decay of rho after that
var_state_levels_persistent2 <- rho[1]/mean(rho/lag(rho), na.rm = TRUE)*var_state_levels
# Calculate the share of variance coming from other transitory sources.
# We will need this to calculate the variance breakup for changes
var_state_levels_other2 <- var_state_levels - var_state_levels_persistent2 - var_state_sampling
# Save a graph showing the decay of the correlation
quick_df <- tibble(lag = seq(0,5), correlation = c(1,rho))
ggplot(quick_df, aes(x= lag, y = correlation)) +
geom_line() +
geom_point() +
ylim(0,1)
ggsave(paste(subject, "- correlation_decay.png"), width = 5, height = 6 , path = output)
# Create empty tibble to store results
# I
df_bar <- tibble(state_or_dist = rep(c("State","District"),each =6),
changes_or_levels =rep(rep(c("Changes","Levels"), each =3), times= 2),
bar_part = factor(rep(c("Persistent","Transitory sampling", "Transitory other"),4), levels = c("Transitory other","Transitory sampling", "Persistent")),
value = rep(.001,12)
)
# Update the values for the district changes bar
df_bar$value[(df_bar$state_or_dist == "District") & (df_bar$changes_or_levels == "Changes") & (df_bar$bar_part == "Persistent")] <- var_dist_deltas - 2*var_dist_epsilon
df_bar$value[(df_bar$state_or_dist == "District") & (df_bar$changes_or_levels == "Changes") & (df_bar$bar_part == "Transitory sampling")] <- 2*var_dist_sampling
df_bar$value[(df_bar$state_or_dist == "District") & (df_bar$changes_or_levels == "Changes") & (df_bar$bar_part == "Transitory other")] <- 2*var_dist_epsilon - 2*var_dist_sampling
# Update the values for the district levels bar
df_bar$value[(df_bar$state_or_dist == "District") & (df_bar$changes_or_levels == "Levels") & (df_bar$bar_part == "Persistent")] <- var_dist_levels - var_dist_epsilon
df_bar$value[(df_bar$state_or_dist == "District") & (df_bar$changes_or_levels == "Levels") & (df_bar$bar_part == "Transitory sampling")] <- var_dist_sampling
df_bar$value[(df_bar$state_or_dist == "District") & (df_bar$changes_or_levels == "Levels") & (df_bar$bar_part == "Transitory other")] <- var_dist_epsilon - var_dist_sampling
##### STATE DATA #####
# CHANGES
df_bar$value[(df_bar$state_or_dist == "State") & (df_bar$changes_or_levels == "Changes") & (df_bar$bar_part == "Persistent")] <- var_state_deltas-2*(var_state_sampling+var_state_levels_other2)
df_bar$value[(df_bar$state_or_dist == "State") & (df_bar$changes_or_levels == "Changes") & (df_bar$bar_part == "Transitory sampling")] <- 2*var_state_sampling
df_bar$value[(df_bar$state_or_dist == "State") & (df_bar$changes_or_levels == "Changes") & (df_bar$bar_part == "Transitory other")] <- 2*var_state_levels_other2
# LEVELS
df_bar$value[(df_bar$state_or_dist == "State") & (df_bar$changes_or_levels == "Levels") & (df_bar$bar_part == "Persistent")] <- var_state_levels_persistent2
df_bar$value[(df_bar$state_or_dist == "State") & (df_bar$changes_or_levels == "Levels") & (df_bar$bar_part == "Transitory sampling")] <- var_state_sampling
df_bar$value[(df_bar$state_or_dist == "State") & (df_bar$changes_or_levels == "Levels") & (df_bar$bar_part == "Transitory other")] <- var_state_levels_other2
# I have created this graph so that you can see it in the notebook
# The final version which combines both subjects, is created in a separate notebook
ggplot(df_bar, aes(fill = bar_part, y=value, x= changes_or_levels)) +
geom_bar(position="stack", stat ="identity") +
facet_grid(~ state_or_dist) +
scale_fill_manual(values = c("red", "orange", "blue"))+
labs(fill = "Variance component", x = "")
ggsave(paste(subject," - variance_decomposition.png"), width = 5, height = 6 , path = output)
write_csv(df_bar, file.path(output, paste(subject, "- bar data.csv")))
df_final <- df_bar %>%
pivot_wider(names_from = "bar_part", values_from= "value") %>%
mutate(total_var = `Persistent`+`Transitory sampling`+`Transitory other`) %>%
mutate(share_pers = `Persistent`/total_var, share_samp = `Transitory sampling`/total_var, share_other = `Transitory other`/total_var)
write_csv(df_final, file.path(output, paste(subject, "- var deco.csv")))
(30/530)^10
(500/530)^10
(500/530)^500
source('~/code/notebooks/temp - ICMR surveillance testing.R', echo=TRUE)
source('~/code/notebooks/temp - ICMR surveillance testing.R', echo=TRUE)
source('~/code/notebooks/temp - ICMR surveillance testing.R', echo=TRUE)
source('~/code/notebooks/temp - ICMR surveillance testing.R', echo=TRUE)
source('~/code/notebooks/temp - ICMR surveillance testing.R', echo=TRUE)
source('~/code/notebooks/temp - ICMR surveillance testing.R', echo=TRUE)
source('~/code/notebooks/temp - ICMR surveillance testing.R', echo=TRUE)
